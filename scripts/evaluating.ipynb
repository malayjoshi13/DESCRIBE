{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFjFR1vIvX4d"
      },
      "source": [
        "**STEP 1) CONNECTING GOOGLE COLAB TO GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6AEQptvvgzZ",
        "outputId": "426ef6e8-b9e7-4336-d7c9-f65d8b963fc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2) IMPORTING LIBRARIES**"
      ],
      "metadata": {
        "id": "j1NzdMu_FctL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKiShG6m_Qh7"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "import datetime, os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import string\n",
        "import glob\n",
        "from tensorflow.keras.layers import add, LSTM, Embedding, Dense, Dropout\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint,EarlyStopping\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3) SETTING UP WORKING PLATFORM**"
      ],
      "metadata": {
        "id": "2gQa7XcVFjdF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drp8mtIynf1I"
      },
      "outputs": [],
      "source": [
        "# Skip this code block if you have already cloned the github repo in your GDrive\n",
        "\n",
        "\n",
        "# Change working directory to your Google Drive\n",
        "os.chdir('/content/drive/MyDrive/')\n",
        "\n",
        "# Cloning the repository in your Google Drive.\n",
        "# If you are doing inference right after doing training then no need to clone as during training process, this GitHub repo is cloned.\n",
        "!git clone https://github.com/malayjoshi13/Describer.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOP!\n",
        "\n",
        "Before moving ahead, open this link https://drive.google.com/drive/folders/13YJjbA-iMBkM6NOEbdYpJs56q4JQ9FQx?usp=sharing. Save a shortcut of this folder (to use in this script later) in \"Describer\" directory which would have just created in your Google Drive after running above command. Now move forward."
      ],
      "metadata": {
        "id": "VjrdBZ8k6pg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change working directory to your cloned repository\n",
        "os.chdir('/content/drive/MyDrive/Describer/')\n",
        "\n",
        "# Enter location of folder inside directory named \"Describer\" in your Google Drive, from where you will fetch trained model and needed files.\n",
        "# Two options:\n",
        "#     \"temporary\" (if evaluating on your own trained model) or\n",
        "#     \"default_model_checkpoint\" (if evaluating on default pre-trained model)\n",
        "chkp_location = \"./default_model_checkpoint/\""
      ],
      "metadata": {
        "id": "rCl0AsMSFpY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaXLhwfCbbsa"
      },
      "source": [
        "**STEP 4) SOME PARAMETERS TO BE USED HERE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQHVbhtDLGRD"
      },
      "source": [
        "max_length=34\n",
        "vocab_size=1652\n",
        "embedding_dim=200\n",
        "image_feature_shape=(2048,)\n",
        "\n",
        "filename = chkp_location+'word_to_index.pkl'\n",
        "word_to_index = load(open(filename, 'rb'))\n",
        "\n",
        "filename1 = chkp_location+'index_to_word.pkl'\n",
        "index_to_word = load(open(filename1, 'rb'))\n",
        "\n",
        "filename2 = chkp_location+'all_captions_GLOVE_embedding.pkl'\n",
        "embedding_matrix = load(open(filename2, 'rb'))\n",
        "\n",
        "# Reading names of testing images present in \"TestImagesName.txt\" file and saving them to \"text_content\" variable\n",
        "with open('dataset/TestImagesName.txt', 'r') as file:\n",
        " text_content = file.read()\n",
        "\n",
        "# Reading all image encodings from \"all_images_encodings.pkl\" file and storing it into \"image_content\" variable\n",
        "image_content = load(open(chkp_location+'all_images_encodings.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDj1uHBjtqJ"
      },
      "source": [
        "**STEP 5) EVALUATING THE TRAINED MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWCGlG91k4iw"
      },
      "source": [
        "**STEP 5.1) Reading names of test images present in \"DevImagesNames.txt\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm9KmO0olIaP"
      },
      "outputs": [],
      "source": [
        "# We make a list \"test_images_name\" which stores names of all test images extracted from \"TestImagesName.txt\" file\n",
        "test_images_name = list()\n",
        "for line in text_content.split('\\n'):\n",
        "    if len(line) < 1:\n",
        "        continue\n",
        "    name = line.split('.')[0]\n",
        "    test_images_name.append(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AEvMYQvuscJ"
      },
      "source": [
        "**STEP 5.2) Seperating encodings of test images from total images encodings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCb0mxZMvBdi"
      },
      "outputs": [],
      "source": [
        "# One by one we pick names of every test images from list \"test_images_name\" (from step 2.1) using \"for k in test_images_name\".\n",
        "# Then we save pairs of \"test_images_name\" and their correponding \"image encodings\" in dictionary \"test_images_encodings\" by using \"k: content[k]\";\n",
        "# here k->name of test image and content[k]->encoding of that test image\n",
        "test_images_encodings = {k: image_content[k] for k in test_images_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAkCKFNVuO1y"
      },
      "source": [
        "**STEP 5.3) Seperating test captions from captions corresponding to total images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1MRMi8SXJEl"
      },
      "outputs": [],
      "source": [
        "# Then we check that from dictionary \"modified_captions\" which we made in (step 2.2),\n",
        "# which all imagename-caption pairs matches to names of test images extracted from \"TestImages.txt\" file.\n",
        "# Those who match are added to \"test_captions\" dictionary\n",
        "\n",
        "test_captions = dict()\n",
        "\n",
        "with open(chkp_location+'processed_captions.txt', 'r') as f:\n",
        "  for line in f.readlines():\n",
        "    bits = line.split()\n",
        "    idd, caps_token = bits[0], bits[1:]\n",
        "    if idd in test_images_name:\n",
        "        if idd not in test_captions:\n",
        "            test_captions[idd] = list()\n",
        "        caps = ' '.join(caps_token)\n",
        "        test_captions[idd].append(caps)\n",
        "# This \"test_captions\" dictionary having test_images_name and their captions pairs looks like:\n",
        "# {'1000268201_693b08cb0e': ['startseq child in pink dress....entry way endseq', 'startseq girl....building endseq', 'startseq little girl....endseq',.....], '1001773457_577c3a7d70': ['startseq black dog and spotted dog are fighting endseq',...}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4b9zXQ6v9K-"
      },
      "source": [
        "**STEP 5.4) Re-initializing the model and loading saved weights to the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbHFI3XZwGY1"
      },
      "outputs": [],
      "source": [
        "# image-encoding pipeline\n",
        "inputs1 = Input(shape=image_feature_shape) #see code-block 2\n",
        "layer1 = Dropout(0.5)(inputs1)\n",
        "layer2 = Dense(256, activation='relu')(layer1)\n",
        "\n",
        "# caption-pipeline\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "layerA = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False, mask_zero=True)(inputs2)\n",
        "layerB = Dropout(0.5)(layerA)\n",
        "layerC = LSTM(256)(layerB)\n",
        "\n",
        "# decoder (feed forward) model\n",
        "merging_point = add([layer2, layerC])\n",
        "activator = Dense(256, activation='relu')(merging_point)\n",
        "outputs = Dense(vocab_size, activation='softmax')(activator)\n",
        "\n",
        "trained_model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\n",
        "#load best weight of trained model\n",
        "trained_model.load_weights(chkp_location+'weights/best.h5')\n",
        "\n",
        "# This \"model.h5\" file has memory of connections between words of training captions corresponnding to each encodings of training images.\n",
        "# this weigths has been memorized during training process.\n",
        "# Using this memory here in test process, we will give input of model encoding of test images and will ask model to predict sequences of words corresponding to given test image.\n",
        "# Then we will compare caption predicted by model to that of caption we already have in test dataset. Then on basis of how close both captions are we will rate performance of model's prediction\n",
        "# in terms of BLEU score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maCyHa94wHa7"
      },
      "source": [
        "**STEP 5.5) Initiating evaluation/test process**<br><br>\n",
        "\n",
        "--> Evaluating using Greedy Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrb0-RH2wH-v"
      },
      "outputs": [],
      "source": [
        "def greedy_search(test_img_encodings):\n",
        "    actual, predicted = list(), list()\n",
        "\n",
        "    for idd, cap_list in test_captions.items():\n",
        "      caption = 'startseq'\n",
        "\n",
        "      for i in range(max_length):\n",
        "        # during first iteration \"hints_of_caption\":- \"startseq\".\n",
        "        # then \"caption.split()\" gives \"startseq\" which is present in \"word_to_index\", thus\n",
        "        # \"hints_of_caption\"=[word_to_index[word]]=[1 (which in real is the label encoding for this word)]\n",
        "        hints_of_caption = [word_to_index[word] for word in caption.split() if word in word_to_index]\n",
        "        # becaue of \"pad_sequences\", \"hints_of_caption\" for first iteration becomes [1, 0, 0,....31 more zeroes], so that \"hints_of_caption\" of every\n",
        "        # iteration becomes of same length equal to \"max_length\", i.e. 34\n",
        "        padded_hints_of_caption = pad_sequences([hints_of_caption], maxlen=max_length)\n",
        "        # then \"validation_images_features[idd]\" stores \"encoding of image\" corresponding to \"idd\" and \"cap_list\"\n",
        "        # then this \"image_encoding\" and half caption consisting of \"startseq\" is given to model as an input and ask him to use his\n",
        "        # \"weights\"/\"memory\" to predict what could be possible word next to \"startseq\"\n",
        "\n",
        "        yhat = trained_model.predict([np.array([test_images_encodings[idd]]), np.array(padded_hints_of_caption)])\n",
        "\n",
        "        # then out of all 1798 (=most_occuring_words in the vocabulary) possible outputs for next word, we choose ouput having highest probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # then we convert this label encoded output, into label decoded output, in simple terms if yhat=[6] then \"index_to_word\" will convert it to yhat=['stairs']\n",
        "        word = index_to_word[yhat]\n",
        "        # then \"caption\" = \"startseq\"+\"stairs\" = \"startseq stairs\"\n",
        "        caption += ' ' + word\n",
        "        # and now as \"word\"!=\"endseq\", thus we again go into \"for loop\" and now \"i\"=2\n",
        "        # and now unlike for i=1 where \"caption\":- \"startseq\", for i=2 \"caption\":- \"startseq stairs\"\n",
        "        # again above process will repeat and using current \"caption\" next word will be predicted.\n",
        "        # this process will keep continuing till either predicted next word called \"word\" is \"endseq\" or we have iterated to all 34 words of a caption\n",
        "        if word == 'endseq':\n",
        "          break\n",
        "\n",
        "      # once \"for loop\" ends for a particular caption and we come out of it, next thing is that we will remove \"startseq\" and \"endseq\" from predicted caption.\n",
        "      # We will do this by spliting variable:- finalz = \"startseq stairs are high endseq\", and then using:- finalz[1:-1], we will get:- finalz= ['stairs', 'are', 'high']\n",
        "      # and then using:- ' '.join(finalz) we will get:- final_captionz = stairs are high\n",
        "      finalz = caption.split()\n",
        "      finalz = finalz[1:-1]\n",
        "      final_captionz = ' '.join(finalz)\n",
        "\n",
        "      # then we will split it again and add all words of that predicted caption in list \"predicted\"\n",
        "      predicted.append(final_captionz.split())\n",
        "      print(predicted)\n",
        "\n",
        "      # and then add all words of actual captions of input image in list \"actual\"\n",
        "      references = [d.split() for d in cap_list]\n",
        "      actual.append(references)\n",
        "      print(actual)\n",
        "\n",
        "      return actual, predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Evaluating using Beam Search"
      ],
      "metadata": {
        "id": "XBC7BW4TYZZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(beam_index, test_img_encodings):\n",
        "    actual, predicted = list(), list()\n",
        "\n",
        "    for idd, cap_list in test_captions.items():\n",
        "\n",
        "        start = [word_to_index[\"startseq\"]]\n",
        "        start_word = [[start, 0.0]]\n",
        "        while len(start_word[0][0]) < max_length:\n",
        "            temp = []\n",
        "            for s in start_word:\n",
        "                par_caps = pad_sequences([s[0]], maxlen=max_length, padding='post')\n",
        "                preds = trained_model.predict([np.array([test_img_encodings[idd]]),np.array(par_caps)], verbose=0)\n",
        "                word_preds = np.argsort(preds[0])[-beam_index:]\n",
        "                # Getting the top <beam_index>(n) predictions and creating a\n",
        "                # new list so as to put them via the model again\n",
        "                for w in word_preds:\n",
        "                    next_cap, prob = s[0][:], s[1]\n",
        "                    next_cap.append(w)\n",
        "                    prob += preds[0][w]\n",
        "                    temp.append([next_cap, prob])\n",
        "\n",
        "            start_word = temp\n",
        "            # Sorting according to the probabilities\n",
        "            start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
        "            # Getting the top words\n",
        "            start_word = start_word[-beam_index:]\n",
        "\n",
        "        start_word = start_word[-1][0]\n",
        "        intermediate_caption = [index_to_word[i] for i in start_word]\n",
        "        final_caption = []\n",
        "\n",
        "        for i in intermediate_caption:\n",
        "            if i != 'endseq':\n",
        "                final_caption.append(i)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        final_caption = ' '.join(final_caption[1:])\n",
        "\n",
        "    # then we will split it again and add all words of that predicted caption in list \"predicted\"\n",
        "    predicted.append(final_caption.split())\n",
        "\n",
        "    # and then add all words of actual captions of input image in list \"actual\"\n",
        "    references = [d.split() for d in cap_list]\n",
        "    actual.append(references)\n",
        "\n",
        "    return actual, predicted"
      ],
      "metadata": {
        "id": "L5AIuZPfYdXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxNz1FjUwKY6"
      },
      "source": [
        "**STEP 5.6) TRAINED MODEL's PERFROMANCE --> BLEU scores**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Evaluating using Greedy Search"
      ],
      "metadata": {
        "id": "3NnN9zBAIiFJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pcv1DX6wLBE",
        "outputId": "28693aa0-6c46-400e-9d7a-c67c65563939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "BLEU-1: 0.795413\n",
            "BLEU-2: 0.666974\n",
            "BLEU-3: 0.581795\n",
            "BLEU-4: 0.399388\n"
          ]
        }
      ],
      "source": [
        "actual_list, predicted_list = greedy_search(test_images_encodings)\n",
        "\n",
        "print('BLEU-1: %f' % corpus_bleu(actual_list, predicted_list, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(actual_list, predicted_list, weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu(actual_list, predicted_list, weights=(0.3, 0.3, 0.3, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu(actual_list, predicted_list, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--> Evaluating using Beam Search"
      ],
      "metadata": {
        "id": "OxtZ4ZiSIkXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_list1, predicted_list1 = beam_search(3, test_images_encodings)\n",
        "\n",
        "print('BLEU-1: %f' % corpus_bleu(actual_list1, predicted_list1, weights=(1.0, 0, 0, 0)))\n",
        "print('BLEU-2: %f' % corpus_bleu(actual_list1, predicted_list1, weights=(0.5, 0.5, 0, 0)))\n",
        "print('BLEU-3: %f' % corpus_bleu(actual_list1, predicted_list1, weights=(0.3, 0.3, 0.3, 0)))\n",
        "print('BLEU-4: %f' % corpus_bleu(actual_list1, predicted_list1, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "O8xIxScTefzO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}