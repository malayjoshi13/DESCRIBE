{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPEuI5aMvGXG"
   },
   "source": [
    "# **IMAGE CAPTIONING : USING CNN AND RNN**\n",
    "\n",
    "https://youtu.be/r31_jjT1JFE<br>\n",
    "https://youtu.be/oaV_Fv5DwUM<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvm5Ns7OvKLa"
   },
   "source": [
    "###**1) HOW I GET TO KNOW IMPORTANCE OF IMAGE CAPTIONING SYSTEM**\n",
    "\n",
    "A month ago, when I was going to a near by local market, I saw a blind man standing a few steps away from me. There was soo much crowd all around him with vechiles parked randomly alongside the road and too many pedestrains and vechiles moving on the road, that he was finding it difficult to cross the road. <br><br>\n",
    "The reason for him to be in this situation was that his \"white-cane\" had limitation of not giving him proper description of parked vechiles and about the movement of pedestrains and vechiles around him.<br>\n",
    "https://drive.google.com/uc?export=view&id=1Qx45H1SIz6NxhbGYbBdYmZhZeVO2zCbT<br><br>\n",
    "Although at that very moment with his consent I helped him to cross the road, but after that incident many questions kept bothering me about \"road-safety\" of blind people, who in absence of some voluntary help might losses a lot of time to get description through their walking-white-can of how things in front of them looks like and if their walking-white-can couldn't able to properly describe the surrounding to blind people, in such cases they often get injured.<br>\n",
    "Another question which disturbed me was that sometimes blind-people have to be \"dependent\" on someone to guide them through some very busy and fast-moving road, which often makes them feel a little-bit uncomfortable.<br><br>\n",
    "Luckily, during the time when I encountered this problem, I was working with neural networks and I was pretty clearly in my hand that these will help me to solve this problem by building an \"automated guiding system\" which is a an ongoing-research topic for many giant tech companies.<br>\n",
    "The role of this system would be to describe each scene and ongoing activities to blind people, thus making it easy for them to walk on road with self-esteem, without wasting time in finding their way and with least probability of getting injured.<br><br>\n",
    "The initial step towards building an \"automated guiding system\" would be to improvise existing \"image captioning systems\" to make them predict more accurate captions for given input images. This will lead towards building an efficient \"automated guiding system\" working on realtime videos (a series of images).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2SZzP4KvM0I"
   },
   "source": [
    "### **2) WHAT IS \"IMAGE CAPTIONING SYSTEM\"** \n",
    "\n",
    "So, the decription which I above mentioned above :- \"*..crowd all around him with vechiles parked randomly alongside the road and too many pedestrains and vechiles moving on the road..*\" , was easy for me to decribe by combined functioning of what scene my eyes captured and accordingly how my brain recognised what all objects were present in that scene (by help of my pre-existing knowledge about identity of objects I saw till now in my life). Then the logical part of my brain generated descriptions/captions of all the things it has identified in that scene, with help of sequenced arrangement of identity of those objects + proper placement of parts of speech (like: pronoun, adjective, tense, etc).<br><br>\n",
    "\n",
    "Now I want my \"image captioning system\" to replicate these actions for a static image.So, this discussion takes us to next point that, how \"image captioning system\" will generate captions for a given static image of a scene to it?\n",
    "<br><br>\n",
    "\n",
    "**WORKING OF \"IMAGE CAPTIONING SYSTEM\"**     \n",
    "So, this system uses two models namely CNN (convolutional neural network) and LSTM (a type of recurrent neural network).<br>\n",
    "https://drive.google.com/uc?export=view&id=1OopRGQeUGffN2XPLC_OcYbyAMSydsKq2<br>\n",
    "\n",
    "The role of CNN during training period is to understand relationship between label/identity of each object (present in training images' label dataset) with respective image of that object(present in training images dataset), and then during testing period to use that analysis of relationship between image with its label/identity (done during training period) to predict label/identity of all test objects (which are not seen by system before).\n",
    "\n",
    "Role of LSTM during training period will be to understand and memorize sequence of words in each training caption and then during testing period to predict caption for the given test image by recalling what all he memorized.<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsVl5XxpqRuW"
   },
   "source": [
    "### **3) TRAINING THE \"IMAGE CAPTIONING SYSTEM\"** \n",
    "\n",
    " During training period, captions are feeded to RNN model in \"pair-wise' manner by help of customized generators.<br>\n",
    "https://drive.google.com/uc?export=view&id=1bbPrDYbKc8TP5OG7IYpXfJdlsiif1yBh<br> \n",
    "For example:- “ Ram is playing in ground ” would be split into 6 input-output pairs to train the model.<br>\n",
    "\n",
    "Now after getting these \"input-output\" pairs from \"data_generator\", we feed first training photo (X1) and first word of training caption (X2) to the model. Now we ask him on basis of this input data, predict second word for the incomplete caption (Y^ which consists of first word of training caption).<br>\n",
    "\n",
    "Once he will predict it, we will match this predicted word to already known second word of training caption (Y). Then we will find cost function (to see how close second word predicted by model is close to actual second word). We will keep on doing backpropagation i.e keep on updating parameters of LSTM layer so as to reach most accurate second word and also updating layers of CNN so as to not leave any possibility that due to CNN's wrong prediction for identity of objects, prediction of second word will not turn to be incorrect, till loss between predicted second word (Y^) and actuall second word(Y) is very low.<br>\n",
    "\n",
    "Then the training photo (X1) and first two words of the training caption (X2) will be provided to the model as input and we'll ask model to predict the third word (Y^). After that we will again find loss and try to minimise it by updating parameters, till prediction of third word is accurate.<br>\n",
    "\n",
    "We will do same till we train our model on all words of first training caption and the respective training image. After this we do the same for all training captions and training images.<br><br>\n",
    "\n",
    "X1(image encoding)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X2(words sequence)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y (actual word)<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startseq&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ram<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startseq, Ram&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startseq, Ram, is&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;playing<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\t                startseq, Ram, is, playing&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;startseq, Ram, is, playing, in&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;ground<br>photo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;startseq, Ram, is, playing, in, ground&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;&nbsp;\t&nbsp;&nbsp;endseq<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jF6pwI8_m2J"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIU8QjTsnXz4"
   },
   "source": [
    "### **4) MOUNTING GOOGLE DRIVE AND IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFjFR1vIvX4d"
   },
   "source": [
    "**STEP 4.1) MOUNTING GOOGLE DRIVE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24283,
     "status": "ok",
     "timestamp": 1610190562695,
     "user": {
      "displayName": "Malay Joshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwuEZFx4BpLgLygZI6c-ezHM3OzHzCsshgGbFtuw=s64",
      "userId": "07941266660135468355"
     },
     "user_tz": -330
    },
    "id": "-6AEQptvvgzZ",
    "outputId": "5456fdd8-77a5-4869-f88f-ecc18edec605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgPS7BFPcffu"
   },
   "source": [
    "**STEP 4.2) IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do9CK7RaceqE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzH9u5rjshkm"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zsx152pvQnZ"
   },
   "source": [
    "### **5)PRE-PROCESSING IMAGES AND CAPTIONS DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ1c6r125E5E"
   },
   "source": [
    "**STEP 5.1) ENCODING ALL IMAGES (TRAINING IMAGES+TEST IMAGES)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2UrCX-DRuTb"
   },
   "outputs": [],
   "source": [
    "#don't do it\n",
    "\n",
    "\n",
    "images = '/content/drive/My Drive/image_caption/Flicker8k_Dataset/'\n",
    "img = glob.glob(images+'*.jpg')\n",
    "\n",
    "model3 = InceptionV3(weights=\"imagenet\")\n",
    "\n",
    "new_input = model3.input\n",
    "hidden_layer = model3.layers[-2].output\n",
    "model_new = Model(new_input, hidden_layer)\n",
    "\n",
    "features = dict()\n",
    "for name in img:\n",
    "  image = load_img(name, target_size=(299, 299))\n",
    "  image = img_to_array(image)\n",
    "  image = np.expand_dims(image, axis=0)\n",
    "  image /= 255.\n",
    "  image -= 0.5\n",
    "  image *= 2.\n",
    "  feature = model_new.predict(image)\n",
    "  \n",
    "  feature = np.reshape(feature, feature.shape[1])\n",
    "\n",
    "  name = name.split('/')[6]\n",
    "  image_id = name.split('.')[0]\n",
    "  features[image_id] = feature\n",
    "  print(len(features))\n",
    "\n",
    "with open(\"/content/drive/My Drive/image_caption/all_images_encodingzz.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(features, encoded_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poNanXO1v1OF"
   },
   "source": [
    "### <font color='darkorange'>**code explanation**</font> <br> \n",
    "step1) First load paths of all images present in \"Flicker8k_Dataset\" into \"images\" (such that \"images\" stores ....'/content/drive/My Drive/image_caption/Flicker8k_Dataset/391020801_aaaae1e42b.jpg'..)<br><br>\n",
    "\n",
    "step2) Then we load the Inception model to find encodings of all training images. For that we have to modify inception model by removing second last layer so that instead of getting probabilty of predictions, we can get the \"thinking\"/\"weights\" of model. <br><br>\n",
    "\n",
    "step3) Then we will one by one take each image byits path, convert it to shape suitable for input of inception, preprocess that image, feed that image to inception model using \"predict\" and then get the \"weights\"/\"encoding\" (like '390992388_d74daee638': array([0.09877205, 0.09038477, 0.25259972, ..., 1.1207666 , 0.49923682,\n",
    "       0.72041094], dtype=float32) ) of each training image which we store in \"features\".<br><br>\n",
    "\n",
    "step4) Then we pickle \"features\" as \"all_images_encoding_testing.pkl\" to be later used.<br><br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtQ1OBE35vgd"
   },
   "source": [
    "**STEP 5.2) CLEANING-UP ALL CAPTIONS (TRAINING CAPTIONS+TEST CAPTIONS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "657lNKPpr2Yl"
   },
   "outputs": [],
   "source": [
    "file = open('/content/drive/My Drive/image_caption/Flickr8k.token.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "\n",
    "captions = dict()\n",
    "for line in content.split('\\n'):\n",
    "    bits = line.split()\n",
    "    if len(line) < 2:\n",
    "        continue\n",
    "    idd, cap = bits[0], bits[1:]\n",
    "    idd = idd.split('.')[0]\n",
    "    cap = ' '.join(cap)\n",
    "    if idd not in captions:\n",
    "        captions[idd] = list()\n",
    "    captions[idd].append(cap)\n",
    "    \n",
    "\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for key, caption_list in captions.items():\n",
    "    for i in range(len(caption_list)):\n",
    "        c = caption_list[i]\n",
    "        c = c.split()\n",
    "        c = [word.lower() for word in c]\n",
    "        c = [w.translate(table) for w in c]\n",
    "        c = [word for word in c if len(word)>1]\n",
    "        c = [word for word in c if word.isalpha()]\n",
    "        caption_list[i] =  ' '.join(c)              \n",
    "        \n",
    "\n",
    "\n",
    "line1 = list()\n",
    "for key, caption_list in captions.items():\n",
    "    for each_cap in caption_list:\n",
    "        line1.append(key + ' ' + each_cap)\n",
    "modified_captions = '\\n'.join(line1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cjIxSO8ByeH"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "step1) First we will open \"Flickr8k.token.txt\" file that has captions for all images(training+test images) in \"content\" (looks like:<br>1000268201_693b08cb0e.jpg#0\tA CHILD .................... stairs in an ENTRY way//<br> \n",
    "1000268201_693b08cb0e.jpg#1\tA girl going into a wooden.....<br>\n",
    "1000268201_693b08cb0e.jpg#2\tA little girl climbing into a .....) <br><br>\n",
    "step2) Then we will take each line/caption from \"content\", split that line and store \"1000268201_693b08cb0e.jpg#1\" in \"idd\" and \"A girl going into a wooden.....\" in \"cap\". Then we will further split \"idd\" and store \"1000268201_693b08cb0e\" in improvised \"idd\". Then we will store all captions related to a particular \"idd\" with it only in foem of \"key-value\" in dictionary \"captions\" (looks like:<br>\n",
    "'1000268201_693b08cb0e': ['A CHILD in a...', 'A girl going ...'......]) <br><br>\n",
    "step3) Then we will clean content in file \"captions\" by making all words to lower case, removing punctuations and removing all characters except alphabetic characters. <br><br>\n",
    "step4) Then we will store all captions along with their image name to \"modified_caption\" named container. (similar format as that of \"Flickr8k.token.txt\") (looks like:<br>\n",
    "1000268201_693b08cb0e child in.......of stairs in an entry way<br>\n",
    "1000268201_693b08cb0e girl.........wooden building<br>\n",
    "1000268201_693b08cb0e little.............playhouse<br>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XNZV5U3r5TR"
   },
   "source": [
    "### **6)SEPERATING TRAINING CAPTIONS TRAINING IMAGES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVbuhAFi6voc"
   },
   "source": [
    "**STEP 6.1) SEPERATING TRAINING CAPTIONS FROM REST ALL CAPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkuAk7VZnLdU"
   },
   "outputs": [],
   "source": [
    "file = open('/content/drive/My Drive/image_caption/Flickr_8k.trainImages.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "\n",
    "training_images_name = list()\n",
    "for line in content.split('\\n'):\n",
    "    if len(line) < 1:\n",
    "        continue\n",
    "    name = line.split('.')[0]\n",
    "    training_images_name.append(name)\n",
    "\n",
    "print(len(training_images_name))\n",
    "\n",
    "training_captions = dict()\n",
    "for line in modified_captions.split('\\n'):\n",
    "    bits = line.split()\n",
    "    idd, caps = bits[0], bits[1:]\n",
    "    if idd in training_images_name:\n",
    "        if idd not in training_captions:\n",
    "            training_captions[idd] = list()\n",
    "        caps = 'startseq ' + ' '.join(caps) + ' endseq'\n",
    "        training_captions[idd].append(caps)\n",
    "\n",
    "print(training_captions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FINDING THE MAXIMUM POSSIBLE LENGTH OF TRAINING CAPTIONS\n",
    "values_of_training_captions1 = list()\n",
    "for key in training_captions.keys():\n",
    "     for values in training_captions[key]:\n",
    "        values_of_training_captions1.append(values) \n",
    "    \n",
    "max_length = max(len(values.split()) for values in values_of_training_captions1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SELECTING AND LABEL-ENCODING MOST OCCURING WORDS OF TRAINING CAPTIONS\n",
    "values_of_training_captions = []\n",
    "for key, val in training_captions.items():\n",
    "    for cap in val:\n",
    "        values_of_training_captions.append(cap)       \n",
    "\n",
    "threshold = 10 \n",
    "counts = {} \n",
    "for each_sentence in values_of_training_captions:\n",
    "    for each_word in each_sentence.split(' '): \n",
    "        counts[each_word] = counts.get(each_word, 0) + 1 \n",
    "most_occuring = [each_word for each_word in counts if counts[each_word] >= threshold]\n",
    "\n",
    "vocab_size = len(most_occuring)+1\n",
    "#print(len(most_occuring))\n",
    "\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "index = 1\n",
    "for each_word in most_occuring:\n",
    "    index_to_word[index] = each_word\n",
    "    word_to_index[each_word] = index\n",
    "    index += 1\n",
    "\n",
    "#print(len(word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHr1ILRIJw9g"
   },
   "outputs": [],
   "source": [
    "#don't run it.....we already have pickeled\n",
    "\n",
    "\n",
    "with open(\"/content/drive/My Drive/image_caption/word_to_index.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(word_to_index, encoded_pickle)\n",
    "\n",
    "with open(\"/content/drive/My Drive/image_caption/index_to_word.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(index_to_word, encoded_pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxdh5657Kb7z"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "step1) First we will load names of all training images.<br><br>\n",
    "step2) Then we will remove \".jpg' from name of each training image and store them in \"training_images_name\".<br><br>\n",
    "step3) Then we will pick captions which are of training image from \"modified_captions\" and store them in \"training_captions\". We will do this by first splitting whole \"modified_captions\" into \"line\", then splitting each \"line\" into \"bits\", then splittinhg each \"bits\" and then storing the image name into \"idd\". Then we will check whether \"idd\" is present inside \"training_images_name\" or not (which has name of all training images)....if \"yes\" then it means that caption present in \"modified_captions\", is a training caption.......................therefore we will save that caption \"cap\" in \"training_captions\".<br><br>In this way we seperate all training captions (and store them in \"training_captions\") out of \"modified_captions\"(having captions of training+test).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JOMPYjcRVhQ"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "step1) There are many words which occurs very less in \"training_captions\" which occurs very less thus are of very less significance for model to train on them as don't givemuch decription for their respective training images. So it's our job to remove those least appearing words.They are located by not fulfilling criteria of occuring even 10 times in any caption. And one very important take-away of this is that we get \"vocab-size\" which tells about total number of training captions' words.<br><br>\n",
    "step2)Now once we have collection of all words which we want to teach to our model so as to generate a caption,i.e. \"most_occuring\", next our job is to encode all these words into vector of numbers (because neurons only understands numbers and not alphabetic characters). We do this by 'label-encoding\" each word, i.e by giving each word a index/number in a dictionary \"word_to_index\",i.e <br>\n",
    "word_to_index = {cat=1<br>\n",
    "dog=2<br>.<br>.<br>...and so on}<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iybAMVn2p9wS"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY7ldwDf69Z6"
   },
   "source": [
    "## <font color='purple'>**learning time**</font><br>\n",
    "Ok so just like we transformed training images into vector of numbers (through encoding) because neurons of deep learning model can only understand \"numbers\", in same way we have to convert **words of training captions** also into vector of \"numbers\".<br>\n",
    "This can be done by two types of encoding methods: **one-hot encoding** and **label encoding**.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Label-encoding**: It will give unique index to each word present in a container. Ex: for the list like :- collection = ['child', 'in', 'pink', 'dress', 'is']', label-encoding will give following encoding result: <br> ['child'=1,<br>\n",
    "'in'=2,<br>\n",
    "'pink'=3,<br>\n",
    "'dress'=4,<br>\n",
    "'is'=5]<br>\n",
    "\n",
    "**used when** : we have **data with large number of categories** (at that time we don't use one-hot as although legth of one-hot and label-encoding will be same but width of one-hot will be more, thus one-hot need more memory) and when **data needs to preserve some order or rank** (like for data with size of t-shirts [XL,L,M,S] or for case of our image-captioning, where sequence of words is important for making a meaningful analyse and thus you can't afford order of encoded vector to be changed or to become random which happens when using one-hot encoding) because label-encoding gives indexes which acts like rank of that word in a list.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**One-hot encoding**: It will place \"1\" at position of word to be encoded, and rest all elements' position will be \"0\". Ex: for the list like:- students=[ram,shyam,sita,gita,hari], one-hot encoding will give following result:<br>\n",
    "        ['ram'=  1  0  0  0  0,<br>\n",
    "       'shyam'=  0  1  0  0  0,<br>\n",
    "        'sita'=  0  0  1  0  0,<br>\n",
    "        'gita'=  0  0  0  1  0,<br>\n",
    "        'hari'=  0  0  0  0  1]<br> \n",
    "\n",
    "**used when**: we have data with **smaller number of categories** (as width of one-hot is very much than label-encoding, thus size problem if data is lengthy....then width and length of one-hot both will bw large, consuming larger memory) and when we can **afford data to be non-sequenced**(like data of names of stuents....where ranking shyam above harior vice-versa doesn't change meaning of data)<br><br>        \n",
    "\n",
    "---\n",
    "Thus we can see that as our data is large and sequence must be there, thus **label encoding** suits us.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNoFKmqiqWkB"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQpdv_jHrLS-"
   },
   "source": [
    "**STEP 6.2) SEPERATING TRAINING IMAGES FROM REST ALL IMAGES-ENCODINGS**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4oY2dS3crH8Y"
   },
   "outputs": [],
   "source": [
    "filename = '/content/drive/My Drive/image_caption/all_images_encoding_final.pkl'\n",
    "all_features = load(open(filename, 'rb'))\n",
    "training_images_features = {k: all_features[k] for k in training_images_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmpIUuFX5lLN"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "\n",
    "step1) Now we call the pickeled file having all images' encodings to our notebook and save these encodings in container \"all_features\" .<br><br>\n",
    "step2) Now we pick name (i.e key) of each training images from \"training_images_name\" and then extract corresponding encoding (i.e value) from dictionary \"all_features\".<br><br>\n",
    "step3) Then we save all these extracted encodings to \"training_images_features\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTTNHK0k-Sv-"
   },
   "source": [
    "**STEP 6.3) USING GloVe TO GENERATE EMBEDDINGS FOR TRAINING CAPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I03fFNd88TA1"
   },
   "outputs": [],
   "source": [
    "file = open('/content/drive/My Drive/image_caption/glove.6B.200d.txt', encoding=\"utf-8\")\n",
    "\n",
    "embedding_of_each_key_word = {}\n",
    "for line in file:\n",
    "    splits = line.split()\n",
    "    key_word = splits[0]\n",
    "    embedding_vector = np.asarray(splits[1:], dtype='float32')\n",
    "    embedding_of_each_key_word[key_word] = embedding_vector\n",
    "\n",
    "file.close()\n",
    "\n",
    "\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in word_to_index.items():\n",
    "    embedding_vector = embedding_of_each_key_word.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tpjd2VElOdwA"
   },
   "outputs": [],
   "source": [
    "#don't run it.....we have already pickeled\n",
    "\n",
    "with open(\"/content/drive/My Drive/image_caption/embedding_matrix.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(embedding_matrix, encoded_pickle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g5VcFg2atl-"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "step1) First we will load \"glove.6B.200d.txt\" (which has already embedded embeddings of soooo many words) in \"file\" named container.<br><br>\n",
    "step2) Then we will split each line present in \"file\", and will store \"name\" associated with each embeddings in \"key_word\" and the corresponding embedding in \"embedding_vector\". Then we save \"key_word\" and \"embedding_vector\" in form of key-value in dictionary called \"embedding_of_each_key_word\".<br><br>\n",
    "step3) Now our next job is to pick-up embeddings only of words that our training captions have. This is done by first creating a zeroed-matrix named \"embedding_matrix\". Then, we will pick each word of our training captions from \"word_to_index\" dictionary and will get their embeddings using GloVe's list of embeddings \"embedding_of_each_key_word\". Then we will save these embeddings of words of training captions \"embedding_vector\" in \"embedding_matrix\".<br><br>\n",
    "\n",
    "This **embedding matrix** will have **words of training captions** along with their **GloVe embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEjviCeGqidq"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mopWLp-lG8c"
   },
   "source": [
    "## <font color='purple'>**learning time**</font><br>\n",
    "Above we learnt about **encoding words of training captions**, but be it one-hot encoding (like:- {sita=[1 0 0], ram=[0 1 0 ], hari=[0 0 1]}) or label-encoding (like:- {sita=1, ram=2, hari=3}).............for both encodings there are some **problems**:<br>\n",
    "1)encoded features are sparse (i.e so many zeroes and single one)<br>\n",
    "2)none of two encodings can tell relationship between the two similar/related words. For eample in encodings of words \"dog\" and \"cat\", there should be somesemantic relationship, which is missing in both one-hot encoding and label encoding.<br>\n",
    "3)The size of encodings increases as the total number of words increases in training captions.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "To solve this problem we use **\"embeddings\"**. These are of two types:<br><br>\n",
    "<font color='blue'>****1)Frequency based Embedding:****</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**1.1)Count vector/ Bag of words:** \n",
    "                        Consider a \"corpus\" C of \"documents\" D1,D2 (i.e D1: Ram is lazy. Ram likes to sleep in the class.<br>  D2: Sita is in class 5th. Sita likes sing the songs.)<br> \n",
    "\n",
    "Now N unique tokens are extracted from D1 and D2 and put in container :<br>\n",
    "[‘He’, ’She’, ’lazy’, 'is', 'a', ’boy’, ’Neeraj’, 'person’, 'studies', 'class', '5th'].<br>\n",
    "\n",
    "Now we prepare a \"Count Vector matrix\" M of size D X N (i.e 2*6). Each row in this matrix contains the frequency of tokens in document D(i). The matrix looks like:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ram&nbsp;&nbsp;lazy&nbsp;&nbsp;class&nbsp;&nbsp;5th&nbsp;&nbsp;is&nbsp;&nbsp;the&nbsp;&nbsp;sleeps&nbsp;&nbsp;in&nbsp;&nbsp;likes&nbsp;&nbsp;Sita&nbsp;&nbsp;sing<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D2&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br>\n",
    "\n",
    "**Advantage:** It has made numerical-representation of each word less sparse.<br>\n",
    "\n",
    "**Disadvantage:**  But still these encoded feature are sparse(like one-hot and label encodings were), no effect on size of numerical-representation and still size will increase with increase of number of words of training captions and still no semantic relationship can be derived between the numerical-representation/ embedding.<br><br>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**1.2)TF-IDF vectorization(term-frequency inverse-document-frequency) :**\n",
    "TF-IDF tries to reduce the dimensionality of above matrix by removing  words like 'is', 'the' and 'in' which although occurs in both D1 and D2 but are of less importance for both D1 and D2 (as D1 is focused around 'Ram', so D1 must give more weight and D2 is around 'Sita', so D2 must give importance to 'Sita').<br>\n",
    "\n",
    "The situation arrived because words like: 'is','the' and 'in' surpasses the condition of being unique due their 'word count' (as occurs very much in D2 and D1)....so to filter them out we have to use TF-IDF which will filter them on basis that they have occured frequently in D1 and D2 (when counted together) but not that much frequently in either D1 or D2.<br>\n",
    "\n",
    "Let's see how TF-IDF works:<br>\n",
    "**TF = (Number of times term t appears in a document)/(Number of terms in the document)**....it denotes the contribution of the word to the document i.e words relevant to the document will be frequent, thus their TF will be higher.<br>\n",
    "TF('the',Document1): 1/10 = 0.1<br>\n",
    "TF('Ram',Document1):2/10  = 0.2<br>\n",
    "\n",
    "**IDF=log(N is the number of documents/n is the number of documents a term t has apeared)**........ if a word has appeared in all the document, then it means that word is not relevant to a particular document. But if it has appeared in a particuar document then it means that word is of some relevance to that documentin which it is present in.<br>\n",
    "IDF('the',Document1):log(2/2) = 0<br>\n",
    "IDF('Ram',Document1):log(2/1) = 0.301<br>\n",
    "\n",
    "**TF-IDF = TF*IDF**<br>\n",
    "TF-IDF('the',Document1):0.1*0 = 0<br>\n",
    "TF-IDF ('Ram',Document1):0.2*0.301 = 0.0602<br><br>\n",
    "                                     \n",
    "**Thus we see that TF-IDF has given preference to document specific word like 'Ram'.**<br>\n",
    "Now filling according to TF-IDF...<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ram&nbsp;&nbsp;lazy&nbsp;&nbsp;class&nbsp;&nbsp;5th&nbsp;&nbsp;is&nbsp;&nbsp;the&nbsp;&nbsp;sleeps&nbsp;&nbsp;in&nbsp;&nbsp;likes&nbsp;&nbsp;Sita&nbsp;&nbsp;sing<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D1&nbsp;&nbsp;0.06&nbsp;&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D2&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03&nbsp;0.03&nbsp;0&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.03<br>\n",
    "                                            \n",
    "Now we will delete features with null value....so as to get reduced dimensional matrix...\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f7<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ram&nbsp;&nbsp;lazy&nbsp;&nbsp;class&nbsp;&nbsp;5th&nbsp;&nbsp;&nbsp;&nbsp;sleeps&nbsp;&nbsp;Sita&nbsp;&nbsp;sing<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D1&nbsp;&nbsp;0.06&nbsp;&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D2&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.03&nbsp;0.03&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.03<br>\n",
    "\n",
    "Then we will train out model on these features (f1,f2,f3,f4,f5,f6,f7).<br><br>\n",
    "**Advantage:** The dimensionality is reduced in comparison to that of \"count vector/words of bag\" embedding method.<br>\n",
    "**Disadvantage:** But still no semantic relationship and sparsity is still there.<br><br>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**1.3)Co-Occurrence Matrix:**\n",
    "Similar words will be placed together – Apple and mango both being fruits will be placed nearly.<br>\n",
    "Let us write co-occurence matrix for below document D3 using window of size '2'.<br><br>\n",
    "D3:- He is not lazy. He is intelligent. He is smart.<br><br>\n",
    "&nbsp;&nbsp;word(down)/context(-->)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;He&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not&nbsp;&nbsp;&nbsp;&nbsp;lazy&nbsp;&nbsp;&nbsp;intelligent&nbsp;&nbsp;smart<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;He&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;not&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lazy&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;intelligent&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;smart&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0<br><br>\n",
    "**Advantage=**Now after applying PCA or SVD, each coloumn(row) of occurence matrix will represent vector embedding representation of context(word) that has preserved semantic relationship in them (will save how close two words are).<br>\n",
    "**Disadvantage=**But still this representation is little bit sparse and no effect on it's dimensionality.<br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color='blue'>****2)Prediction based Embedding:****</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**2.1)Word2Vec:** It produces embeddings of words by either using **CBOW** (continous bag of words) or **SKIPGRAM**. On one hand CBOW has aim of finding embedding of \"target word\", by help of his neighbouring \"context words\".....while, SKIPGRAM finds embeddings of all neighbouring \"context words\", by help of \"target word\". <br>https://drive.google.com/uc?export=view&id=1Hrr4gLiDntNTcCQ3tzYw5qX86q6XvSTg<br>https://drive.google.com/uc?export=view&id=1wslpzs9Aaqx1vuv1RKPuyUpp-M6BT-fn<br><br>\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "**2.2)GloVe:**Predicting by help of neural networks makes Word2vec a slower process. Also word2vec only works locally around \"target and context words\" thus don't produce good analogy (which is produced if working if all the words at once). Thus we use GloVe which is a blend of Word2Vec (providing feature of local relationship, thus good sense of what comes after what) and TF-IDF (providing global feature of good analogy as generalise the similarity between words).<br>https://drive.google.com/uc?export=view&id=1JL6JWtbV36J7RGXx0NusIGF4qzJ5Ca7R<br>https://drive.google.com/uc?export=view&id=193GxQdicmO0ND64N9r61qHzb6HUCPgzf<br>https://drive.google.com/uc?export=view&id=1N8k5D1FYwh21C8eCdMfrVFbs23iu_AMw<br>https://drive.google.com/uc?export=view&id=1nCmmRGtdfsSIxM4dObNNDWPFin-EhL-W<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHu7hUVjn-St"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGDU7IOe_10F"
   },
   "source": [
    "### **7) CREATING \"DATA_GENERATOR\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L5QTr04ALRJ"
   },
   "outputs": [],
   "source": [
    "def data_generator(training_captions, training_images_features, word_to_index, max_length, num_photos_per_batch):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    n=0\n",
    "\n",
    "    while 1:\n",
    "        for key, cap_list in training_captions.items():\n",
    "            n+=1\n",
    "            encoded_image = training_images_features[key]\n",
    "\n",
    "            for cap in cap_list:\n",
    "                encoded_caption = [word_to_index[word] for word in cap.split(' ') if word in word_to_index]\n",
    "\n",
    "                for i in range(1, len(encoded_caption)):\n",
    "                    input_encoded_partial_caption, output_encoded_partial_caption = encoded_caption[:i], encoded_caption[i]\n",
    "                    input_encoded_partial_caption = pad_sequences([input_encoded_partial_caption], maxlen=max_length)[0]\n",
    "                    output_encoded_partial_caption = to_categorical([output_encoded_partial_caption], num_classes=vocab_size)[0]\n",
    "                    X1.append(encoded_image)\n",
    "                    X2.append(input_encoded_partial_caption)\n",
    "                    y.append(output_encoded_partial_caption)\n",
    "\n",
    "            if n==num_photos_per_batch:\n",
    "                yield ([array(X1), array(X2)], array(y))\n",
    "                X1, X2, y = list(), list(), list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjTKJs3UxevS"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "All pre-processed training images and training captions will now flow down to \"Data_generator\". Then the whole of the data will be breaked and converted into pair-wise inputs (X1,X2) and output (Y). Then on the command of \"model.compile\", these pair-wise data will flow into \"structure of model\" in form of inputs1= \"X1\" and inputs2= \"X2\" and outputs= \"Y\" (will be used to find cost function aolng with predicted label Y^.)<BR><BR>\n",
    "\n",
    "So, for now let's see how \"data_generator\" is making these \"pair-wise\" data.<br>\n",
    "\n",
    "**step1)** First of all \"generator\" present in section 5.1 and 5.2, is collecting all the pre-processed data we worked upon till now. Then he sends \"data_generator\" all those data. **Note initially n= 0**<br><br>\n",
    "\n",
    "**step2)** In first iteration of \"for key, cap_list in training_captions.items():\",  \"data_generator\" extracts name of first training image (like: 1000268201_693b08cb0e) and store it in \"key\" and then extracts all 5 captions for first training image (like: A little girl climbing into a ....) and store it in \"cap_list\". Then'll pick-up corresponding encoding of the first training image. **Note: n= 1**<br><br>\n",
    " \n",
    "**step3)** Then in first iteration of \"for cap in cap_list:\",  \"data_generator\" will pick first caption (out of all 5 captions for first training image/ \"key\"). Then it will encode first caption using  \"word_to_index\" dictionary and store encoded first caption in \"encoded_caption\". <br>\n",
    "\n",
    " For example let's assume  that first caption given by \"cap\" is:- [startseq Ram is sleeping endseq]. This when encoded and stored in \"encoded_caption\" becomes:- [(index of startseq in \"word_to_index\" dictionary), (index of Ram in \"word_to_index\" dictionary), (index of is in \"word_to_index\" dictionary), (index of sleeping in \"word_to_index\" dictionary), (index of endseq in \"word_to_index\" dictionary)], i.e. [2, 3, 4, 8, 14]<br><br>\n",
    "\n",
    "**step4)** Now for 1st iteration for loop \"for i in range(1, len(encoded_caption))\":-<br>\n",
    " input_encoded_partial_caption= [2] and output_encoded_partial_caption= 3.<br>\n",
    "Will be added in X1, X2, y \n",
    " <br><br>\n",
    "\n",
    "Now for 2nd iteration for loop \"for i in range(1, len(encoded_caption))\":-<br>\n",
    " input_encoded_partial_caption= [2, 3] and output_encoded_partial_caption= 4.<br>\n",
    "Will be added in X1, X2, y \n",
    " <br><br>\n",
    "\n",
    " Now for 3rd iteration for loop \"for i in range(1, len(encoded_caption))\":-<br>\n",
    " input_encoded_partial_caption= [2, 3, 4] and output_encoded_partial_caption= 8.<br>\n",
    " Will be added in X1, X2, y \n",
    " <br><br>\n",
    "\n",
    "Now for 4th iteration for loop \"for i in range(1, len(encoded_caption))\":-<br>\n",
    " input_encoded_partial_caption= [2,3,4,8] and output_encoded_partial_caption= 14.<br>\n",
    " Will be added in X1, X2, y<br><br> \n",
    "\n",
    "**step5)** With this **\"for i in range(1, len(encoded_caption))\" ENDS**, now cursor move back and **starts second iteration of \"for cap in cap_list:\"**. There it will pick-up second caption (out of all 5 captions for first training image/ \"key\"), encode it and save in \"encoded_caption\". Then again all iterations of \"for i in range(1, len(encoded_caption))\" will run and will generate \"input-output\"pairs which will be added to X1, X2 and y.<br>\n",
    "Repeating this process will generate \"input-output\" pairs for all 5 captions of first training image will be generated and added to X1, X2 and y, which looks like:<br><br>\n",
    "\n",
    "**step6)** After **completing** for all **5 captions of first training image**, cursor move up and **starts second iteration of \"for key, cap_list in training_captions.items():\"** (as a result of which **n becomes 2**). There it will pick-up second training image and it's all 5 captions. Encode image. Then one-by-one picking each of 5 captions, generate \"input-output\" pair of each caption and then finally adding encodings of second image and \"input-output\" pairs of all 5 captions of second image to X1,X2 and y.<br><br>\n",
    "\n",
    "**step7)** **Repeating** this process for third image and it's caption. And then as **n becomes 3**, thus **X1, X2 and y** (having data of first 3 training images and captions) will be **sent to structure of model** for training.<br>\n",
    "Here **X1, X2 and y** will look like:<br><br>\n",
    "\n",
    "X1:- ([@, @, @....., @] <--first member of X1 <--first training image<br>\n",
    "[@, @, @....., @] <--second member of X1 <--second training image<br>\n",
    "[@, @, @....., @] <--third member of X1 <--third training image<br><br>\n",
    "\n",
    "X2:- ([2], [2,3], [2,3,4], [2,3,4,8] <--first member of X2 <--related to first caption of first training image<br>\n",
    "[bla], [bla,bla], [bla,bla,bla] <--second member of X2 <--related to second caption of first training image<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "[bla], [bla,bla], [bla,bla,bla], [bla,bla,bla,bla], [bla,bla,bla,bla,bla] <--fifteenth member of X2 <--fifth caption of third training image]<br><br>\n",
    "\n",
    "y:- ([3],[4],[8],[14] <--first member of y <--related to first caption of first training image<br>\n",
    "[bla],[bla],[14] <--first member of y <--related to first caption of first training image<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "[bla], [bla], [bla], [bla], [bla], [14] <--fifteenth member of y <--related to fifth caption of third training image]<br><br>\n",
    "\n",
    "\n",
    "**step8)** Once set of three images + their captions is sent to \"structure of model\", then the whole above process will be done for next three set of training images + their captions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE2q2A6dAeDx"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvITWW0YASDr"
   },
   "source": [
    "### **8) CREATING STRUCTURE OF TRAINING MODEL**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8085,
     "status": "ok",
     "timestamp": 1607800814510,
     "user": {
      "displayName": "Malay Joshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwuEZFx4BpLgLygZI6c-ezHM3OzHzCsshgGbFtuw=s64",
      "userId": "07941266660135468355"
     },
     "user_tz": -330
    },
    "id": "IEq3zLZsApwa",
    "outputId": "a3840bf0-20a2-4a5c-f62d-0ce012cc0cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 200)      330400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 200)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1652)         424564      dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,813,268\n",
      "Trainable params: 1,482,868\n",
      "Non-trainable params: 330,400\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs1 = Input(shape=(2048,))\n",
    "layer1 = Dropout(0.5)(inputs1)\n",
    "layer2 = Dense(256, activation='relu')(layer1)\n",
    "\n",
    "\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "layerA = Embedding(vocab_size, 200, mask_zero=True)(inputs2)\n",
    "layerB = Dropout(0.5)(layerA)\n",
    "layerC = LSTM(256)(layerB)\n",
    "\n",
    "\n",
    "layer = add([layer2, layerC])\n",
    "layerr = Dense(256, activation='relu')(layer)\n",
    "outputs = Dense(vocab_size, activation='softmax')(layerr)\n",
    "\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\n",
    "\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W4680Pe7NXW"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "**step1)** Let's understand \"structure of model\", by understanding how \"data\" will interact with each layer of this structure.<br>\n",
    "**Let's start understanding from code-line \"model = Model(inputs=[inputs1, inputs2], outputs=outputs)\"**. This line will pass 3 training data at a time from \"data_generator\" to here {3 images + their 5 each captions, in form of **\"input-output\" pairs (inputs1= X1, inputs2= X2 and outputs= y)**}, when commanded by \"model.compile\" (will see in next code template).<br><br>\n",
    "\n",
    "**step2)** Let's consider \"inputs1\"= first member of list X1= [@, @, @....., @] . Now, \"inputs1\" will go to first part of model which will **give his understanding about encoding of first training image input to it.**  And on other hand \"inputs2\"= second element of first member of list X2= [2,3]. \"inputs2\" will go to next part of model which will **give his understanding about embedding (2nd layer converted input encoding into embedding) of partial caption input to it** <br>\n",
    "\n",
    "We did one improvisation that **instead of using \"Embedding layer\"** we used embeddings already picked from **GloVe's collection**.<br><br>\n",
    "\n",
    "**step3)** Then we combine both understandings and predict word \"y^\" which should next to 3 in partial caption [2,3]. Now we find cost function \"L\" using predicted word (i.e y^) and target word of \"4\" (i.e y).<br><br>\n",
    "\n",
    "**step4)** Then to minimise this loss function (so that predicted word is nearly equal to actual word ) we use \"model.compile\" which has parameters through which we will do backpropagation/ optimization and measure the loss/ difference between predicted and actual words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvIDWlwbril5"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RP5aDxJhR2y"
   },
   "source": [
    "## <font color='purple'>**learning time**</font><br>\n",
    "\n",
    "# **RNN** (RECURRENT NEURAL NETWORK)\n",
    "\n",
    "1) RNN is generalised form of \"feed-forward NN/ Artificial NN\" where O/P from previous step is fed as I/P to current step.https://drive.google.com/uc?export=view&id=1Hrr4gLiDntNTcCQ3tzYw5qX86q6XvSTg<br><BR>https://drive.google.com/uc?export=view&id=1AOPYDI96RvyPeLRBwBy4-H9cA9phf7e6 <br><br>\n",
    "\n",
    "2) It is designed to recognize sequential flow of a data and to use that sequential pattern to predict next likely scenario. For example Artifical NN can solve task of distinguishing between African and American person on basis of their pics....but can't predict the ending of a movie, because it can't understand sequences between previous data (like movie's starting and the climax) and pesent data(movie's ending).<br>\n",
    "To understand this, RNN will be used which will understand the sequential patterns between starting and climax, and based on it, it will predict the \"movie's ending\".<br><br>\n",
    "\n",
    "3) But problem with RNN is that if sequenced data is long, then number of units in RNN will be more. Because of this during backprop when multiple times derivative of loss function will happen, dL/dw will soo small when it reaches to first unit, that updation in weights will be very minor(i.e. w:=w-dL/dw will be updated very lessly)......thus learning process will be very slow...this is called \"vanishing gradient\".<br><br>\n",
    "\n",
    "4) To overcome this problem we use LSTM, which uses \"memory\" to carry important content till very last step of even a long sequenced data.<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osfC13XzlUen"
   },
   "source": [
    "# **LSTM** (LONG SHORT TERM MEMORY)\n",
    "\n",
    "1) So role of LSTM in model structure is that it will understand sequencing of each word in input X2= [2, 3, 4]= [encoding of 'startseq', encoding of 'ram', encoding of 'is'] (and for time being let's **forgets** that this input will be feeded along with image wncoding to predict next word y^= encoding of 'sleeping'= 8).<br><br>\n",
    "\n",
    "2)let's see how it do so:-<br>\n",
    "https://drive.google.com/uc?export=view&id=12QbMCy1JzcJ-Nsk1SXPn0E0kQY3UaoVT<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=105bA4iPFvFpUzBMu4OE9Go2rsKc9xunL<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=1K7A9b4PT-NvU4AsMLeQZi4jp6iuVWgqu<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=1uFxYyLLbd38e49EpbPnpOdUYszXWwahO<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=1za1Dw3TqVnv2MuVEyGf7EK9cMUhp4f9B<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=1PX7Tiw5508P5SX-t9BGMObAh3dqB3d6P<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=13uzh3KJ8klug8VwfxkMDmkOMrG4a3nIc<br><br>\n",
    "\n",
    "https://drive.google.com/uc?export=view&id=1FQSiCVLbJsei_SZtptIk7RLlfN1VDNDw<br><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xogrv8GWrk8X"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O03gFrCsBQBM"
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "iterations = 30\n",
    "number_pics_per_batch = 36\n",
    "steps = len(training_captions)//number_pics_per_batch\n",
    "\n",
    "for i in range(iterations):\n",
    "    generator = data_generator(training_captions, training_images_features, word_to_index, max_length, number_pics_per_batch)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGeskqld2RlB"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "We will train model for epoch= 1 (i.e for 1 time model will go forward and backpropagation on whole training dataset).<br>\n",
    "\n",
    "Now in this one-time whole running we will extract and train model on training data of batch size 3 (i.e 3 images + their captions coming from 'data_generator') for total numbers of 2000 steps (so as to train on whole of 6000 data).<br>\n",
    "\n",
    "Now epoch of 1 will not update parameters so much so we repeat this step for 20 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tq0igxqqufh"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "\n",
    "We will train model for epoch= 1 (i.e for 1 time model will go forward and backpropagation on whole training dataset).<br>\n",
    "\n",
    "Now in this one-time whole running we will extract and train model on training data of batch size 3 (i.e 3 images + their captions coming from 'data_generator') for total numbers of 2000 steps (so as to train on whole of 6000 data).<br>\n",
    "\n",
    "Now epoch of 1 will not update parameters so much so we repeat this step for 10 times.<br>\n",
    "\n",
    "And this time we will backpropagation with learning rate of 0.0001 which was earlier 0.001 ( because now we are moving towards hitting minima, thus we don't want to get deviated by using large learning step).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWGacfJ4BggF"
   },
   "outputs": [],
   "source": [
    "#don't run as we already have saved weights file             \n",
    "\n",
    "\n",
    "model.save_weights('/content/drive/My Drive/image_caption/modelzzz.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idr-j8hTCHX1"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sm2D4q4ete5P"
   },
   "source": [
    "### **9) EVALUATION TIME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAkCKFNVuO1y"
   },
   "source": [
    "**STEP 9.1) SEPERATING VALIDATION CAPTIONS FROM REST ALL CAPTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1MRMi8SXJEl"
   },
   "outputs": [],
   "source": [
    "file = open('/content/drive/My Drive/image_caption/Flickr_8k.testImages.txt', 'r')\n",
    "content = file.read()\n",
    "file.close()\n",
    "\n",
    "validation_images_name = list()\n",
    "validation_images_path = list()\n",
    "for line in content.split('\\n'):\n",
    "    if len(line) < 1:\n",
    "        continue\n",
    "    validation_images_path.append(line)    \n",
    "    name = line.split('.')[0]\n",
    "    validation_images_name.append(name)\n",
    "\n",
    "\n",
    "\n",
    "validation_captions = dict()\n",
    "for line in modified_captions.split('\\n'):\n",
    "    bits = line.split()\n",
    "    idd, caps = bits[0], bits[1:]\n",
    "    if idd in validation_images_name:\n",
    "        if idd not in validation_captions:\n",
    "            validation_captions[idd] = list()\n",
    "        caps = 'startseq ' + ' '.join(caps) + ' endseq'\n",
    "        validation_captions[idd].append(caps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJq3FVV9udK"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "step1) First we will load names of all validation images.<br><br>\n",
    "step2) Then we will remove \".jpg\" from name of each training image and store them in \"validation_images_name\".<br><br>\n",
    "step3) Then we will pick captions which are of training image from \"modified_captions\" and store them in \"validation_captions\". We will do this by first splitting whole \"modified_captions\" into \"line\", then splitting each \"line\" into \"bits\", then splittinhg each \"bits\" and then storing the image name into \"idd\". Then we will check whether \"idd\" is present inside \"validation_images_name\" or not (which has name of all training images)....if \"yes\" then it means that caption present in \"modified_captions\", is a validation caption.......................therefore we will save that caption \"cap\" in \"validation_captions\".<br><br>In this way we seperate all validation captions (and store them in \"validation_captions\") out of \"modified_captions\"(having captions of training+test).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AEvMYQvuscJ"
   },
   "source": [
    "**STEP 9.2) SEPERATING VALIDATION IMAGE-CAPTIONS FROM REST ALL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCb0mxZMvBdi"
   },
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "\tall_features = load(open(filename, 'rb'))\n",
    "\tfeatures = {k: all_features[k] for k in dataset}\n",
    "\treturn features\n",
    "\n",
    "validation_images_features = load_photo_features('/content/drive/My Drive/image_caption/all_images_encoding_final.pkl', validation_images_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWA8I9lt_F2M"
   },
   "source": [
    "<font color='darkorange'>**code explanation**</font><br>\n",
    "\n",
    "step1) Now we call the pickeled file having all images' encodings to our notebook and save these encodings in container \"all_features\" .<br><br>\n",
    "\n",
    "step2) Now we pick name (i.e key) of each validation images from \"validation_images_name\" and then extract corresponding encoding (i.e value) from dictionary \"all_features\".<br><br>\n",
    "\n",
    "step3) Then we save all these extracted encodings to \"validation_images_features\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4b9zXQ6v9K-"
   },
   "source": [
    "**STEP 9.3) LOADING THE SAVED MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbHFI3XZwGY1"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/My Drive/image_caption/model_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maCyHa94wHa7"
   },
   "source": [
    "**STEP 9.4) INITIATING VALIDATION PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hrb0-RH2wH-v"
   },
   "outputs": [],
   "source": [
    "#greedy search\n",
    "\n",
    "\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for idd, cap_list in validation_captions.items():\n",
    "    caption = 'startseq'\n",
    "\n",
    "    for i in range(max_length):\n",
    "      sequence = [word_to_index[w] for w in caption.split() if w in word_to_index]\n",
    "      sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "      hii = validation_images_features[idd]\n",
    "      yhat = model.predict([np.array([validation_images_features[idd]]), np.array(sequence)])\n",
    "\n",
    "      yhat = np.argmax(yhat)\n",
    "      word = index_to_word[yhat]\n",
    "\n",
    "      caption += ' ' + word\n",
    "      \n",
    "      if word == 'endseq':\n",
    "        break\n",
    "    finalz = caption.split()\n",
    "    finalz = finalz[1:-1]\n",
    "    final_captionz = ' '.join(finalz)\n",
    "  \n",
    "    yhat = final_captionz \n",
    "    predicted.append(yhat.split())\n",
    "\n",
    "    references = [d.split() for d in cap_list]\n",
    "    actual.append(references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxNz1FjUwKY6"
   },
   "source": [
    "**STEP 9.5) SHOWING THE BLEU SCORES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2965,
     "status": "ok",
     "timestamp": 1607801334754,
     "user": {
      "displayName": "Malay Joshi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiwuEZFx4BpLgLygZI6c-ezHM3OzHzCsshgGbFtuw=s64",
      "userId": "07941266660135468355"
     },
     "user_tz": -330
    },
    "id": "9Pcv1DX6wLBE",
    "outputId": "b282a774-e353-4f56-975b-5e69bbb71985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.474329\n",
      "BLEU-2: 0.290639\n",
      "BLEU-3: 0.203119\n",
      "BLEU-4: 0.098985\n"
     ]
    }
   ],
   "source": [
    "print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training_non_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
